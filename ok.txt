import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)
svm_classifier.fit(X_train, y_train)

# Calculate accuracy
y_pred = svm_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
-------------------------------------------------------------------------------------------------------
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# Generate random data for testing
np.random.seed(0)  # for reproducibility
data = np.random.rand(100, 2)
df = pd.DataFrame(data, columns=['X', 'Y'])

# Save DataFrame to CSV file
df.to_csv('test_data.csv', index=False)

print("CSV file 'test_data.csv' has been generated successfully.")
data = pd.read_csv('test_data.csv')
X = data.values




# K-means clustering
k = KMeans(3)
k.fit(X)
klabel = k.labels_
kcenter = k.cluster_centers_

# EM clustering
em = GaussianMixture(3)
em.fit(X)
elabel = em.predict(X)
ecenter = em.means_

# Compare clustering results
print("K-means labels:")
print(klabel)
print("EM labels:")
print(elabel)

# Visualize clustering results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=klabel, cmap='viridis')
plt.scatter(kcenter[:, 0], kcenter[:, 1], marker='*', s=300, c='r')
plt.title('K-means Clustering')

plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=elabel, cmap='viridis')
plt.scatter(ecenter[:, 0], ecenter[:, 1], marker='*', s=300, c='r')
plt.title('EM Clustering')

plt.show()

# Silhouette Score for K-means
ksil = silhouette_score(X, klabel)
print(f"K-means Silhouette Score: {ksil}")

# Silhouette Score for EM (GMM)
esil = silhouette_score(X, elabel)
print(f"EM (GMM) Silhouette Score: {esil}")
------------------------------------------------------------------------------------------------
import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination

data = pd.read_csv("heart.csv")

model = BayesianNetwork([
    ('Age', 'HeartDisease'), 
    ('Gender', 'HeartDisease'),
    ('ChestPainType','HeartDisease'), 
    ('ExerciseInducedAngina','HeartDisease'),
    ('HeartDisease','RestingECG'),
    ('HeartDisease','Cholesterol')
])


model.fit(data, estimator=MaximumLikelihoodEstimator)
cpd_HeartDisease = MaximumLikelihoodEstimator(model, data).estimate_cpd('HeartDisease')
inference = VariableElimination(model)

printable = inference.query(
    variables=['HeartDisease'], 
    evidence={'RestingECG': 1}
    )
print(printable)
---------------------------------------------------------------------------------------------------
import numpy as np
import pandas as pd

# Loads data
data = pd.read_csv("play_tennis.csv")
# Removes the column "Day" as it is not relevant to the model
data.drop(["day"], axis=1, inplace=True)

def naive_bayes_predict(data, target_name, test_instance):
    valueCounts = data[target_name].value_counts().to_dict()
    pvalue = (data[data.columns[data.shape[1] - 1]].value_counts()/data.shape[0]).to_dict()    
    pred = {}
    for target, _ in valueCounts.items():
        attribute = 1
        subset = data[data[target_name] == target]
        for attr, value in test_instance.items():
            condProb = subset[subset[attr] == value].shape[0] / subset.shape[0]
            attribute =attribute* condProb
        prob = pvalue[target] * attribute       
        pred[target] = prob
        
    return pred

    # Creates a test instance
test_instance = {"outlook": "Sunny", 
                 "temp": "Cool",
                 "humidity": "High",
                 "wind": "Strong"}
# Performs prediction
prediction = naive_bayes_predict(data.copy(), data.columns[data.shape[1] - 1], test_instance)
print("Prediction is:",prediction)
------------------------------------------------------------------------------------------
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler

class NeuralNetwork:
    def __init__(self, ips, hds, ops):
        np.random.seed(42)
        self.wi = np.random.rand(ips, hds)
        self.wh = np.random.rand(hds, ops)
        self.bh = np.random.rand(hds)
        self.bo = np.random.rand(ops)

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)

    def forward_propagation(self, inputs):
        self.hip = np.dot(inputs, self.wi) + self.bh
        self.hop = self.sigmoid(self.hip)        
        self.fip = np.dot(self.hop, self.wh) + self.bo
        self.fop = self.sigmoid(self.fip)        
        return self.hop, self.fop

    def backpropagation(self, inputs, hop, fop, expop, lRate):
        operr = expop - fop
        opdel = operr * self.sigmoid_derivative(fop)        
        hderr = opdel.dot(self.wh.T)
        hddel = hderr * self.sigmoid_derivative(hop)        
        self.wh += hop.T.dot(opdel) * lRate
        self.wi += inputs.T.dot(hddel) * lRate
        self.bo += np.sum(opdel, axis=0) * lRate        
        self.bh += np.sum(hddel, axis=0) * lRate

    def train(self, inputs, expop, lRate, epochs):
        for epoch in range(epochs):
            hop, fop = self.forward_propagation(inputs)
            self.backpropagation(inputs, hop, fop, expop, lRate)

    def predict(self, inputs):
        _, fop = self.forward_propagation(inputs)
        return fop

    def accuracy(self, predictions, labels):
        pred_labels = np.argmax(predictions, axis=1)
        true_labels = np.argmax(labels, axis=1)
        return np.mean(pred_labels == true_labels)

# Load and preprocess the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target.reshape(-1, 1)

encoder = OneHotEncoder(sparse_output=False)
scaler = StandardScaler()
X = scaler.fit_transform(X)
y = encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the network
nn = NeuralNetwork(ips=4, hds=5, ops=3)
nn.train(X_train, y_train, lRate=0.1, epochs=10000)

# Test the network
test_results = nn.predict(X_test)
print("Test Accuracy:", nn.accuracy(test_results, y_test))
