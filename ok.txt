import matplotlib.pyplot as plt 

hours_spent_studying = [10, 9, 2, 15, 10, 16, 11, 16] 
final_exam_scores = [80, 75, 30, 90, 85, 95, 85, 90] 
 
plt.figure(figsize=(8, 6)) 
plt.plot(hours_spent_studying, final_exam_scores, 'r*-', markersize=10) 
plt.title('Effect of Hours Spent Studying on Final Exam Score') 
plt.xlabel('Number of Hours Spent Studying') 
plt.ylabel('Score in the Final Exam (0-100)') 
plt.grid(True) 
plt.show() 
-------------------------------------
import pandas as pd 
import matplotlib.pyplot as plt 
 

mtcars = pd.read_csv("/content/sample_data/mtcars.csv") 
 
plt.figure(figsize=(8, 6)) 
plt.hist(mtcars['mpg'], bins=10, color='skyblue', edgecolor='black') 
plt.title('Frequency Distribution of Miles per Gallon (mpg)') 
plt.xlabel('Miles per Gallon (mpg)') 
plt.ylabel('Frequency') 
plt.grid(True) 
plt.show() 
----------------------------------------------
import pandas as pd
import numpy as np

url = "/content/sample_data/BL-Flickr-Images-Book.csv"
df = pd.read_csv(url)

print("Original Data:")
print(df.head())

irrelevant_columns = [
    'Edition Statement', 'Corporate Author', 'Corporate Contributors',
    'Former owner', 'Engraver', 'Contributors', 'Issuance type', 'Shelfmarks'
]
df.drop(columns=irrelevant_columns, inplace=True)

print("\nData after Dropping Irrelevant Columns:")
print(df.head())

df.set_index('Identifier', inplace=True)

print("\nData after Setting New Index:")
print(df.head())

df['Date of Publication'] = df['Date of Publication'].str.extract(r'(\d{4})').astype(float)

print("\nData after Tidying 'Date of Publication':")
print(df.head())

df['Place of Publication'] = np.where(df['Place of Publication'].str.contains('London'), 'London', df['Place of Publication'])
df['Place of Publication'] = np.where(df['Place of Publication'].str.contains('Oxford'), 'Oxford', df['Place of Publication'])

print("\nData after Cleaning 'Place of Publication':")
print(df.head())
--------------------------------------------------------------
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression(C=1e4, max_iter=200, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f'Classification Accuracy: {accuracy:.4f}')
----------------------------------------------------------------
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train SVM models with RBF kernel, gamma=0.5, and varying C values
C_values = [0.01, 1, 10]
best_accuracy = 0
best_model = None
best_C = None
support_vectors_count = 0

for C in C_values:
    model = SVC(kernel='rbf', gamma=0.5, C=C)
    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    num_support_vectors = len(model.support_)

    print(f'C={C}, Accuracy={accuracy:.4f}, Number of Support Vectors={num_support_vectors}')

    # Track the best model
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = model
        best_C = C
        support_vectors_count = num_support_vectors

print(f'\nBest Model: C={best_C}, Best Accuracy={best_accuracy:.4f}, Number of Support Vectors={support_vectors_count}')
-----------------------------------------------------------------------------------------------------------------------------
import pandas as pd
import numpy as np

# Step 1: Calculate Entropy
def entropy(target_col):
    elements, counts = np.unique(target_col, return_counts=True)
    entropy_value = np.sum([-counts[i] / np.sum(counts) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])
    return entropy_value

# Step 2: Calculate Information Gain
def info_gain(data, split_attribute_name, target_name="Profitable"):
    total_entropy = entropy(data[target_name])
    vals, counts = np.unique(data[split_attribute_name], return_counts=True)
    
    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name]) for i in range(len(vals))])
    information_gain = total_entropy - weighted_entropy
    return information_gain

# Step 3: ID3 Algorithm to build the tree
def ID3(data, original_data, features, target_attribute_name="Profitable", parent_node_class=None):
    if len(np.unique(data[target_attribute_name])) <= 1:
        return np.unique(data[target_attribute_name])[0]
    elif len(data) == 0:
        return np.unique(original_data[target_attribute_name])[np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])]
    elif len(features) == 0:
        return parent_node_class
    else:
        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]
        item_values = [info_gain(data, feature, target_attribute_name) for feature in features]
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]
        
        tree = {best_feature: {}}
        features = [i for i in features if i != best_feature]
        
        for value in np.unique(data[best_feature]):
            sub_data = data.where(data[best_feature] == value).dropna()
            subtree = ID3(sub_data, original_data, features, target_attribute_name, parent_node_class)
            tree[best_feature][value] = subtree
        
        return tree

# Step 4: Load the dataset
data = {
    'Price': ['Low', 'Low', 'Low', 'Low', 'Low', 'Med', 'Med', 'Med', 'Med', 'High', 'High', 'High', 'High'],
    'Maintenance': ['Low', 'Med', 'Low', 'Med', 'High', 'Med', 'Med', 'High', 'High', 'Med', 'Med', 'High', 'High'],
    'Capacity': [2, 4, 4, 4, 4, 4, 4, 2, 5, 4, 2, 2, 5],
    'Airbag': ['No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes'],
    'Profitable': ['Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes']
}

df = pd.DataFrame(data)

# Step 5: Run the ID3 algorithm
features = list(df.columns[:-1])
tree = ID3(df, df, features)

import pprint
pprint.pprint(tree)
--------------------------------------------------------------------------------------------------------
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.cluster import KMeans, AgglomerativeClustering 
from sklearn.metrics import adjusted_rand_score 

# Load the dataset
data = pd.read_csv("/content/sample_data/Spiral.txt", delimiter="\t") 

# Visualize the dataset
plt.figure(figsize=(8, 6)) 
plt.scatter(data['f1'], data['f2'], c=data['label'], cmap='viridis', marker='o') 
plt.title('Dataset Visualization') 
plt.xlabel('f1') 
plt.ylabel('f2') 
plt.show() 

# Extract features
X = data[['f1', 'f2']].values 

# Apply K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42) 
kmeans_labels = kmeans.fit_predict(X) 

# Apply Single-link Hierarchical Clustering
single_link = AgglomerativeClustering(n_clusters=3, linkage='single') 
single_link_labels = single_link.fit_predict(X) 

# Apply Complete-link Hierarchical Clustering
complete_link = AgglomerativeClustering(n_clusters=3, linkage='complete') 
complete_link_labels = complete_link.fit_predict(X) 

# Calculate Rand Index for each method
true_labels = data['label'].values 
rand_index_kmeans = adjusted_rand_score(true_labels, kmeans_labels) 
rand_index_single = adjusted_rand_score(true_labels, single_link_labels) 
rand_index_complete = adjusted_rand_score(true_labels, complete_link_labels) 

print(f'Rand Index for K-means Clustering: {rand_index_kmeans:.4f}') 
print(f'Rand Index for Single-link Hierarchical Clustering: {rand_index_single:.4f}') 
print(f'Rand Index for Complete-link Hierarchical Clustering: {rand_index_complete:.4f}') 

# Visualize the clustering results

# K-means Clustering Results
plt.figure(figsize=(8, 6)) 
plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', marker='o') 
plt.title('K-means Clustering') 
plt.xlabel('f1') 
plt.ylabel('f2') 
plt.show() 

# Single-link Hierarchical Clustering Results
plt.figure(figsize=(8, 6)) 
plt.scatter(X[:, 0], X[:, 1], c=single_link_labels, cmap='viridis', marker='o') 
plt.title('Single-link Hierarchical Clustering') 
plt.xlabel('f1') 
plt.ylabel('f2') 
plt.show() 

# Complete-link Hierarchical Clustering Results
plt.figure(figsize=(8, 6)) 
plt.scatter(X[:, 0], X[:, 1], c=complete_link_labels, cmap='viridis', marker='o') 
plt.title('Complete-link Hierarchical Clustering') 
plt.xlabel('f1') 
plt.ylabel('f2') 
plt.show() 

